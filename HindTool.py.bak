import AL_General_lib as gl
import Hindcast_plot_lib as hc_plt

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy as sc
import inspect
import datetime
import os
import shutil
import argparse
import subprocess
from warnings import simplefilter
import pickle
import re
import random
# <editor-fold desc="Startup">

simplefilter(action="ignore", category=pd.errors.PerformanceWarning)

pd.options.mode.chained_assignment = None  # default='warn'

script_name = os.path.basename(__file__)

parser = argparse.ArgumentParser(description=script_name)
parser.add_argument('-i', metavar='path_in', required=False, type=str, default='Input.txt',
                    help='the filepath to the input file, if empty "Input.txt"')
parser.add_argument('-o', metavar='path_out', required=False, type=str,
                    help='the filepath to the output dir, if empty, taken from Input file')

args = parser.parse_args()

path_in = args.i

filename = inspect.getframeinfo(inspect.currentframe()).filename
path_main = os.path.dirname(os.path.abspath(filename))

DATA_OUT = {}

Colors = {
    'JBO_grey': '#8e8778',
    'JBO_green': '#008f85',
    'Black': '#000000',
    'Pink': '#FF00FF',
    'White': '#FFFFFF',
    'Aqua': '#00FFFF',
}


# </editor-fold>

# <editor-fold desc="General Functions">

def modify_lua_variables(file_path, variables):
    with open(file_path, 'r') as file:
        lines = file.readlines()
    for variable_name, new_value in variables.items():
        variable_pattern = re.compile(rf'^(\s*{variable_name}\s*=\s*).*?(\s*,\s*--.*)?$')

        for i, line in enumerate(lines):
            match = variable_pattern.match(line)
            if match:
                indentation = match.group(1)
                rest_of_line = match.group(2) if match.group(2) else ','
                lines[i] = f"{indentation}{new_value}{rest_of_line}\n"
                break

    with open(file_path, 'w') as file:
        file.writelines(lines)


def Series_to_txt(series, path):
    with open(path, "w") as text_file:
        for time, row in series.items():
            string = f"{time}" + '\t' + f"{row}" + '\n'
            text_file.write(string)
    return


def DataFrame_to_JBOST_HsTp(df, path):
    with open(path, "w") as text_file:
        for index, row in df.iterrows():
            string = str()
            string += 'os_Hindcast{'

            string += f'{row.index[0]} = {round(row.iloc[0])}, '
            string += f'{row.index[1]} = {row.iloc[1]:.2E}, '
            string += f'{row.index[2]} = {row.iloc[2]:.2E}, '
            string += f'{row.index[3]} = {round(row.iloc[3], 1)}'

            string += '}\n'

            text_file.write(string)
    return


def angles(mode, N, start, width):
    ang = []
    ang_mod = []
    if mode == 'section':

        for _, _ in enumerate(range(N)):
            start.append(start + width)
            ang_mod.append([start % 360, (start + width) % 360])
            start = start + width, 360

    if mode == 'full':

        width = 360 / N

        for i, _ in enumerate(range(N)):
            ang.append([start, start + width])
            ang_mod.append([start % 360, (start + width) % 360])
            start = start + width

    return ang, ang_mod


def create_dictionaries(name_list):
    dictionaries = {}

    for name in name_list:
        dictionaries[f'angle: {name[0]}° to {name[1]}°'] = {}

    return dictionaries


def calc_base(x, y, grid, Input):
    global INFO_LOG, segments, segments_mod

    exp = create_dictionaries(segments)

    # Winkel festlegen und Experimente erstellen

    INFO_LOG += 'regression:    method: ' + str(Input["reg_model"]) + ', degree: ' + str(
        Input["deg_regression"]) + ', percent of result covered by mean: ' + str(
        Input["cut_reg"]) + ', weighting regression by number of data points in bin (1=full): ' + str(
        Input["reg_weighting"]) + '\n'

    linelims = Input["line_plot_zone"]
    reg_zone = Input["reg_zone"]

    if reg_zone[1] is None:
        reg_zone = [reg_zone[0], max(x)]

    if 'weight' not in Input:
        weight = False
    else:
        weight = Input["weight"]

    i = 0

    for ck, cd in exp.items():

        INFO_LOG += '\n' + ck + ': \n'

        n_bin = len(grid) - 1
        x_zone = [min(grid), max(grid)]

        cd['grid'] = {}

        cd['grid'] = gl.grid_pointcloud_in_x(
            DATA_SEC[ck][x.name], DATA_SEC[ck][y.name], grid, weight=weight)

        cd['grid']['isData'] = 1
        cd['grid'].loc[cd['grid']['count'] <= Input['bin_min'], 'isData'] = 0

        nanMask = np.array(cd['grid']['isData'])

        # 95% Grenze

        N_dict = np.size(DATA_SEC[ck][x.name])
        N_upper = round(Input["cut_reg"] / 100 * N_dict)

        cd['grid']['bool_upper'] = 0

        x_points_sorted = DATA_SEC[ck].sort_values(x.name)

        if N_upper != N_dict:
            vm_lim_upper = min(x_points_sorted.iloc[N_upper:-1][x.name])

            _, edges, vs_bin = sc.stats.binned_statistic(
                vm_lim_upper, vm_lim_upper, statistic='count', bins=n_bin, range=x_zone)

            cd['grid'].loc[cd['grid'].index[vs_bin[0]:], 'bool_upper'] = 1

        # mindesdatenpunkte in Bin
        if len(Input["percentiles"]) > 0:
            percentiles(cd['grid'],
                        percent=Input["percentiles"])

        # regression    
        # regressionsbereich
        cd['grid']['bool_reg_zone'] = 0

        cd['grid'].loc[(cd['grid']['x'] > reg_zone[0]) & (
                cd['grid']['x'] < reg_zone[1]), 'bool_reg_zone'] = 1

        # regression plotbereich
        cd['grid']['bool_reg_plot'] = 0

        if linelims[1] is None:

            nanMask_temp = nanMask.copy()
            nanMask_temp[min(np.where(nanMask == 1)[0]):max(np.where(nanMask == 1)[0])] = 1

            cd['grid'].loc[(cd['grid']['x'] > linelims[0]) & (
                    nanMask_temp == 1), 'bool_reg_plot'] = 1
        else:
            cd['grid'].loc[(cd['grid']['x'] > linelims[0]) & (
                    cd['grid']['x'] < linelims[1]), 'bool_reg_plot'] = 1

        col_key = [
            col for col in cd['grid'].columns if 'mean' in col or 'percentile' in col]

        cd['log'] = {}

        # mask with bereich
        x_reg_zone = cd['grid'].loc[(
                                            cd['grid']['bool_reg_zone'] == 1) & (nanMask == 1), 'x']

        weights = cd['grid'].loc[(cd['grid']['bool_reg_zone'] == 1) & (
                nanMask == 1), 'count']

        for name in col_key:

            if len(x_reg_zone) != 0:

                y_reg_zone = cd['grid'].loc[(
                                                    cd['grid']['bool_reg_zone'] == 1) & (nanMask == 1), name]

                reg_model = gl.model_regression(x_reg_zone, y_reg_zone, degree=Input["deg_regression"],
                                                method=Input["reg_model"],
                                                weights=weights, weights_regulation=Input["reg_weighting"],
                                                reg_free_n=Input["reg_free_n"])

                cd['grid'][f'{name} regression'] = float('nan')

                pred_reg = gl.predict_regression(reg_model, cd['grid']['x'])

                cd['grid'][f'{name} regression'] = pred_reg

                cd['log']['regression_coeffs'] = [
                    reg_model[2].intercept_, reg_model[2].coef_]

                INFO_LOG += name + '\n' + '    coefficents [a_1 .. a_n]: ' + str(
                    reg_model[2].coef_) + '\n    interception [a_0]: ' + str(reg_model[2].intercept_) + '\n'

            else:
                cd['grid'][f'{name} regression'] = float('nan')
                INFO_LOG += name + '\n no data in regression area'

            cd['grid'][f'{name} result'] = cd['grid'][name]

            cd['grid'].loc[cd['grid']['bool_upper'] == 1,
            f'{name} result'] = cd['grid'].loc[cd['grid']['bool_upper'] == 1, f'{name} regression']

            cd['grid'][f'{name} result plot'] = float('nan')

            cd['grid'].loc[cd['grid']['bool_reg_plot'] == 1,
            f'{name} result plot'] = cd['grid'].loc[cd['grid']['bool_reg_plot'] == 1, f'{name} result']
        i = i + 1
    return exp


def calc_quantile(Data_Out, Input):
    """" expects 2 percentiles and HSTP Data """
    for ck, cd in Data_Out.items():
        col_key = [
            col for col in cd['grid'].keys() if 'result' in col]
        key_perc_mean = [col for col in col_key if 'mean' in col][0]
        try:

            T_up = 1 / Input['quant_up']
            T_low = 1 / Input['quant_low']

            percentile_low = str(Input["percentiles"][0])
            percentile_up = str(Input["percentiles"][1])

            key_perc_up = [col for col in col_key if percentile_up in col][0]
            key_perc_down = [
                col for col in col_key if percentile_low in col][0]

            nans = np.empty(len(cd["grid"]))
            nans[:] = np.nan

            aether = pd.Series(nans)

            temp = cd["grid"][key_perc_up]

            LOW = cd["grid"].loc[(temp < T_low), key_perc_up]

            temp = cd["grid"][key_perc_mean]

            MIDDLE = cd["grid"].loc[(temp >= T_low) & (
                    temp <= T_up), key_perc_mean]

            temp = cd["grid"][key_perc_down]

            UP = cd["grid"].loc[temp > T_up, key_perc_down]

            LOW_MIDDLE = [
                value for value in LOW.index if value in MIDDLE.index]
            MIDDLE_UP = [value for value in MIDDLE.index if value in UP.index]
            LOW_UP = [value for value in LOW.index if value in UP.index]

            LOW = LOW.drop(index=LOW_MIDDLE)

            UP = UP.drop(index=MIDDLE_UP)

            MIDDLE = MIDDLE.drop(index=LOW_UP)

            df_with_nan = pd.concat([LOW, MIDDLE, UP])

            df_with_nan = df_with_nan.combine_first(pd.DataFrame(aether))

            CONNECT_LOW = df_with_nan.loc[(cd["grid"][key_perc_mean] < (T_low + T_up) / 2) & df_with_nan.isna()]

            CONNECT_LOW[:] = T_low
            CONNECT_UP = df_with_nan.loc[(cd["grid"][key_perc_mean] > (T_low + T_up) / 2) & df_with_nan.isna()]

            CONNECT_UP[:] = T_up

            concat = pd.concat(
                [LOW, CONNECT_LOW, MIDDLE, CONNECT_UP, UP])

            cd["grid"]["quantile"] = concat

            cd["grid"].loc[cd["grid"]["bool_reg_plot"]
                           != 1, 'quantile'] = float('nan')
        except:
            print(
                f"    quantile not possible for segment {ck}, check if graph is monotone in 'line_plot_zone' or if percentiles cross in the frequency band. quantile is set to mean")
            cd["grid"]["quantile"] = cd["grid"][key_perc_mean]
            cd["grid"].loc[cd["grid"]["bool_reg_plot"]
                           != 1, 'quantile'] = float('nan')

    return


def percentiles(df, percent: list):
    for n_perc, perc in enumerate(percent):
        z = sc.stats.norm.ppf(perc / 100)

        df[f'{perc}th percentile'] = df["mean"] + z * df["std"]
    return


def read_input_txt(file_path):
    data_dicts = []
    dict_names = []
    cd = {}
    dict_num = 0
    DICT_OUT = {}

    with open(file_path, 'r') as file:
        for line in file:

            # auskommentieren
            line = line.split('#', 1)[0]
            # Check for quotation marks to start a new dictionary

            if '*' in line:

                # Extract the name between quotation marks

                section_name = line[line.index('*') + 1:line.rindex('*')]
                dict_names.append(section_name)
                # Save the current dictionary and start a new one
                if dict_num != 0:
                    data_dicts.append(cd)

                cd = {}
                dict_num = dict_num + 1
            else:
                # Split the line into key and value using one or more spaces
                parts = line.split('>')

                for i, part in enumerate(parts):
                    parts[i] = part.strip()

                # Ensure the line is not empty
                if np.size(parts) > 1:
                    key = parts[0]
                    value = parts[1]
                    try:
                        value = eval(value)
                    except (SyntaxError, NameError, TypeError) as e:
                        print(
                            f"    WARNING: {e}, Error in Input File Value '{key}' in Section '{section_name}', entry set to 'None'")
                        value = None
                    # Add the key-value pair to the current dictionary
                    cd[key] = value

    # Append the last dictionary after reading the file
    data_dicts.append(cd)

    i = 0
    for name in dict_names:
        DICT_OUT[name] = data_dicts[i]
        i = i + 1

    return DICT_OUT


def Data_out_csv(Data_Out, Name, path_csv):
    global path_out

    i = 0

    for dict_dict_name, cd_dict in Data_Out.items():
        dict_dict_name = dict_dict_name.replace(':', '=')
        dict_dict_name = dict_dict_name.replace(' ', '')

        i = i + 1
        i_print = str(i).zfill(2)

        path_csv_full = path_csv + Name + '_' + i_print + '_' + dict_dict_name + '.csv'

        cd_dict.to_csv(path_csv_full)


def Data_out_xls(Data_Out, Name, path_csv):
    global path_out

    i = 0

    with pd.ExcelWriter(path_csv + Name) as writer:
        for dict_dict_name, cd_dict in Data_Out.items():
            dict_dict_name = dict_dict_name.replace(':', '=')
            dict_dict_name = dict_dict_name.replace(' ', '')

            cd_dict.to_excel(writer, sheet_name=dict_dict_name, index=False)

            i = i + 1


# </editor-fold>

# <editor-fold desc="Data Read">
# %% Data Read
def DATA_metocean_read(Input):
    global INFO_LOG

    df_wave_NAN = pd.read_csv(Input["path_wave"], skiprows=15, na_filter=False)
    df_wind_NAN = pd.read_csv(Input["path_wind"], skiprows=15, na_filter=False)
    df_wave = df_wave_NAN.dropna(how='any')
    df_wind = df_wind_NAN.dropna(how='any')
    DataEval["NAN"]["wave_data"] = len(df_wave_NAN) - len(df_wave)
    DataEval["NAN"]["wind_data"] = len(df_wind_NAN) - len(df_wind)

    df_wave.set_index('datetime (ISO 8601) [UTC]', inplace=True)
    df_wind.set_index('datetime (ISO 8601) [UTC]', inplace=True)

    df_wind.index = pd.to_datetime(df_wind.index)
    df_wave.index = pd.to_datetime(df_wave.index)

    df_wind_resample = df_wind.resample(INPUT_SELECT["dt_sample"]).mean()
    df_wave_resample = df_wave.resample(INPUT_SELECT["dt_sample"]).mean()

    range_outer = [min(df_wind.index[0], df_wave.index[1]),
                   max(df_wind.index[-1], df_wave.index[-1])]
    range_inner = [max(df_wind.index[0], df_wave.index[1]),
                   min(df_wind.index[-1], df_wave.index[-1])]
    DataEval["timestamps"]["winddata"] = [df_wind.index[0], df_wind.index[-1]]
    DataEval["timestamps"]["wavedata"] = [df_wave.index[0], df_wave.index[-1]]
    DataEval["timestamps"]["range_inner"] = range_inner
    DataEval["timestamps"]["range_outer"] = range_outer

    df_ges = df_wind_resample.join(df_wave_resample, how='inner')

    return df_ges


def DATA_ABP_mer_read(Input):
    global INFO_LOG

    df = pd.read_csv(Input["path_data"],
                     sep=Input["Seperation"], low_memory=False)
    df = df.drop(0)

    df_datetime = df[df.columns[[0, 1, 2, 3, 4]]]

    df = df.astype(np.float64)

    df.index = pd.to_datetime(df_datetime)
    df = df.resample(INPUT_SELECT["dt_sample"]).mean()

    df_used = df.dropna()

    N_used = len(df_used)

    N_excl = len(df) - len(df_used)

    perc_exclude = N_excl / len(df)

    range_time = [df.index[0], df.index[-1]]

    INFO_LOG += 'ABP_mer Data: \n   Data found (#' + str(N_used) + '): ' + str(
        range_time)

    INFO_LOG += f'excluded Data because Nan: {N_excl} (' + str(
        perc_exclude) + '%), (happens likely because of the seperation of the Jonswap-Spectra in Swell- and Windsea)'

    return df_used


def DATA_general_read(Input):
    global INFO_LOG

    df = pd.read_csv(Input["path_data"],
                     sep=Input["Seperation"], low_memory=False)
    df = df.drop(df.index[Input["drop_rows"]])

    if Input["DateTime_mode"] == "standard":
        df_datetime = df[df.columns[[0]]]

    if Input["DateTime_mode"] == "seperated":
        df_datetime = df[df.columns[[0, 1, 2, 3, 4]]]

    df = df.astype(np.float64)

    df.index = pd.to_datetime(df_datetime)
    df = df.resample(INPUT_SELECT["dt_sample"]).mean()

    df_used = df.dropna()

    N_used = len(df_used)

    N_excl = len(df) - len(df_used)

    perc_exclude = N_excl / len(df)

    range_time = [df.index[0], df.index[-1]]

    INFO_LOG += 'General Data: \n   Data found (#' + str(N_used) + '): ' + str(
        range_time)

    INFO_LOG += 'excluded Data because Nan: {N_excl} (' + str(
        perc_exclude) + '%)'

    return df_used


# </editor-fold>

# <editor-fold desc="Module Functions">

# %% VMHS Funktion
def calc_VMHS(data, Input):
    x = data[COLNAMES["Vm"]]
    y = data[COLNAMES["Hs"]]

    # Grid festlegen
    grid = np.linspace(CALCLIMS["Vm"][0], CALCLIMS["Vm"][1], Input["N_grid"] + 1)

    VMHS_DATA = calc_base(x, y, grid, Input)

    return VMHS_DATA


# %% HSTP Funktion
def calc_HSTP(data, Input):
    x = data[COLNAMES["Hs"]]
    y = data[COLNAMES["Tp"]]

    # Grid festlegen
    grid = np.linspace(CALCLIMS["Hs"][0], CALCLIMS["Hs"][1], Input["N_grid"] + 1)

    HSTP_DATA = calc_base(x, y, grid, Input)

    if Input["quantile"]:
        calc_quantile(HSTP_DATA, Input)

    return HSTP_DATA


# %% VMTP Funktion
def calc_VMTP(Input, Data_Out):
    VMTP = create_dictionaries(segments)

    VMHS = Data_Out["VMHS"]
    HSTP = Data_Out["HSTP"]

    for (ck_VMHS, cd_VMHS), (ck_HSTP, cd_HSTP) in zip(VMHS.items(), HSTP.items()):

        VMTP[ck_VMHS]['grid'] = pd.DataFrame()

        mask = cd_VMHS["grid"]["bool_reg_plot"] == 1

        HS = cd_VMHS['grid']['mean result']

        nans = np.empty(len(HS))
        nans[:] = np.nan

        VM_grid = pd.Series(nans)

        VM_grid[mask] = cd_VMHS['grid'].loc[mask, 'x']

        HS_grid = cd_HSTP['grid']['x']

        VM_res = np.interp(HS_grid, HS[mask], VM_grid[mask],
                           left=float('nan'), right=float('nan'))

        if Input["HSTP"]["quantile"]:
            key_tp = "quantile"
        else:
            key_tp = "mean result"

        VMTP[ck_VMHS]['grid']['x'] = VM_res
        VMTP[ck_VMHS]['grid'].loc[~np.isnan(
            VM_res), 'mean plot'] = cd_HSTP['grid'].loc[~np.isnan(VM_res), key_tp]

        # VMTP[ck_VMHS]['points'] = pd.concat(
        #     [cd_VMHS['points'][COLNAMES["Vm"]], cd_HSTP['points'][COLNAMES["Tp"]], cd_HSTP['points'][COLNAMES["angle"]]], axis=1)

    return VMTP


# %% RosePlot

def calc_RosePlot(data, Input):
    exp = create_dictionaries(segments)

    # Limits x
    r_zone = np.array(Input["r_zone"], dtype=np.float64)

    # Gewünschte Daten filtern und in einzelne Dataframes schreiben
    if Input["col_name_angle"] is None:
        Input["col_name_angle"] = COLNAMES["angle"]

    data = gl.filter_dataframe(data, [Input["col_name_r"]], [
        r_zone[0]], [r_zone[1]])

    r = data[Input["col_name_r"]]

    step = Input["r_section_step"]
    # TODO:
    # Grid festlegen
    if np.isnan(r_zone[1]):
        r_zone[1] = max(r)

    r_zone_len = r_zone[1] - r_zone[0]

    N_inner = int(np.floor(r_zone_len / step))

    r_edges = np.linspace(r_zone[0], N_inner * step, N_inner + 1)

    if r_edges[-1] != r_zone[1]:
        r_edges = np.append(r_edges, max(r))

    i = 0

    for ck, cd in exp.items():
        cd['grid'] = pd.DataFrame()

        cd['points'] = gl.filter_dataframe(
            DATA, [INPUT_SELECT["col_name_angle"]], [segments_mod[i][0]], [segments_mod[i][1]])

        cd['grid']['r_uper_edge'] = r_edges[1:]
        i = i + 1

        cd['grid']['count'], _, _ = sc.stats.binned_statistic(
            cd['points'][r.name], cd['points'][r.name], statistic='count', bins=r_edges)

        cd['grid']['prob'] = cd['grid']['count'] / len(DATA) * 100

    return exp


# %% RWI Funktion


def calculate_RWI(_, Input):
    RWI = {}

    for ck, cd in DATA_SEC.items():

        RWI_list = []
        for HS, TP in zip(DATA_SEC[ck][COLNAMES["Hs"]], DATA_SEC[ck][COLNAMES["Tp"]]):
            S = gl.JONSWAP(Input["f_0"], TP, HS)[0]

            RWI_list.append(np.sqrt(S))

        RWI[ck] = pd.Series(RWI_list, name="RWI", index=cd.index)

    return RWI


# %% Tables Funktion


def calc_tables(Data_Out, Input):
    VMHS = Data_Out["VMHS"]
    VMTP = Data_Out["VMTP"]

    # Grid table
    vm_zone = np.array(Input["Tables"]["Vm_zone"], dtype=np.float64)
    vm_step = Input["Tables"]["Vm_step"]

    if np.isnan(vm_zone[1]):
        vm_zone[1] = max(DATA[COLNAMES["Vm"]])

    vm_length = vm_zone[1] - vm_zone[0]

    N_inner = int(np.ceil(vm_length / vm_step) + 1)

    Vm_table_edges = np.linspace(
        vm_zone[0], vm_zone[0] + (N_inner - 1) * vm_step, N_inner)

    Hsdata = pd.DataFrame()

    Tpdata = pd.DataFrame()
    isDATA_VMHS = pd.DataFrame()
    isDATA_VMTP = pd.DataFrame()
    Count = pd.DataFrame()

    VMZone_VMHS = {}
    VMZone_VMTP = {}

    Hsdata['v_hub'] = [
        f"{Vm_table_edges[i]} to < {Vm_table_edges[i + 1]}" for i in range(len(Vm_table_edges) - 1)]
    Tpdata['v_hub'] = [
        f"{Vm_table_edges[i]} to < {Vm_table_edges[i + 1]}" for i in range(len(Vm_table_edges) - 1)]
    isDATA_VMHS['V_hub'] = [
        f"{Vm_table_edges[i]} to < {Vm_table_edges[i + 1]}" for i in range(len(Vm_table_edges) - 1)]
    isDATA_VMTP['V_hub'] = [
        f"{Vm_table_edges[i]} to < {Vm_table_edges[i + 1]}" for i in range(len(Vm_table_edges) - 1)]
    Count['V_hub'] = [
        f"{Vm_table_edges[i]} to < {Vm_table_edges[i + 1]}" for i in range(len(Vm_table_edges) - 1)]

    # VMHS
    i = 0
    for ck, cd in DATA_SEC.items():
        # VM and HS data from Regression curves
        HS_VMHS = VMHS[ck]["grid"]["mean result plot"]
        VM_VMHS = VMHS[ck]["grid"]["x"]

        # get VM grid and count infomation for Table
        df = gl.grid_pointcloud_in_x(
            DATA_SEC[ck][COLNAMES["Vm"]], DATA_SEC[ck][COLNAMES["Hs"]], Vm_table_edges)
        count_table = df["count"]

        # 'isdata' for table where count != 0
        Vm_table_center = df["x"]
        isdata = count_table != 0
        count_table = np.array(count_table)

        # set span between fist and last isdata==True to True to catch values in between
        spanned_isdata_table = np.empty(len(Hsdata['v_hub']), dtype=bool)
        spanned_isdata_table[:] = False
        spanned_isdata_table[min(
            np.where(isdata)[0]):max(np.where(isdata)[-1] + 1)] = True

        HS = np.empty(len(Hsdata['v_hub']))
        HS[:] = np.nan
        # interplate in span for the non nan values of the regression data
        HS[spanned_isdata_table] = sc.interpolate.interp1d(
            VM_VMHS[~np.isnan(HS_VMHS)], HS_VMHS[~np.isnan(HS_VMHS)], fill_value='extrapolate')(
            Vm_table_center.iloc[spanned_isdata_table])

        vm_zone_VMHS = [Vm_table_center[spanned_isdata_table].iloc[0] -
                        vm_step / 2, Vm_table_center[spanned_isdata_table].iloc[-1] + vm_step / 2]

        VMZone_VMHS[ck] = vm_zone_VMHS

        Hsdata[ck] = HS
        isDATA_VMHS[ck] = isdata
        Count[ck] = count_table

        i = i + 1

    # VMTP
    i = 0

    for ck, cd in DATA_SEC.items():
        # VM and TP data from cross correlation, VM has no leading nans and starts late, not gridded,
        # TP gridded and starts with leading nans

        TP_VMTP = VMTP[ck]["grid"]["mean plot"]
        VM_VMTP = VMTP[ck]["grid"]["x"]

        # get VM grid and count infomation for Table
        df = gl.grid_pointcloud_in_x(
            DATA_SEC[ck][COLNAMES["Vm"]], DATA_SEC[ck][COLNAMES["Hs"]], Vm_table_edges)

        # 'isdata' for table where count != 0
        count_table = df["count"]
        Vm_table_center = df["x"]
        isdata = count_table != 0

        # 'isdata' for table where there is no data because of no cross correlation
        bool_corr = (Vm_table_center > min(VM_VMTP[~np.isnan(VM_VMTP)]) - vm_step / 2) & (
                Vm_table_center < (max(VM_VMTP[~np.isnan(VM_VMTP)]) + vm_step / 2))
        isdata_table = isdata & bool_corr

        # set span between fist and last isdata==True to True to catch values in between

        spanned_isdata_table = np.empty(len(Hsdata['v_hub']), dtype=bool)
        spanned_isdata_table[:] = False
        spanned_isdata_table[min(
            np.where(isdata_table)[0]):max(np.where(isdata_table)[-1] + 1)] = True

        TP = np.empty(len(Hsdata['v_hub']))
        TP[:] = np.nan
        TP[spanned_isdata_table] = sc.interpolate.interp1d(
            VM_VMTP[~np.isnan(TP_VMTP)], TP_VMTP[~np.isnan(TP_VMTP)], fill_value='extrapolate')(
            Vm_table_center.iloc[spanned_isdata_table])

        vm_zone_VMTP = [Vm_table_center[spanned_isdata_table].iloc[0] -
                        vm_step / 2, Vm_table_center[spanned_isdata_table].iloc[-1] + vm_step / 2]

        VMZone_VMTP[ck] = vm_zone_VMTP

        Tpdata[ck] = TP
        isDATA_VMTP[ck] = isdata_table

        i = i + 1

    OUT = {"VMHS": {}}
    OUT["VMHS"]["table_content"] = Hsdata
    OUT["VMHS"]["VM_Zone"] = VMZone_VMHS
    OUT["VMHS"]["isData"] = isDATA_VMHS

    OUT["VMTP"] = {}
    OUT["VMTP"]["table_content"] = Tpdata
    OUT["VMTP"]["VM_Zone"] = VMZone_VMTP
    OUT["VMTP"]["isData"] = isDATA_VMTP

    OUT["count"] = Count
    OUT["Vm_edges"] = Vm_table_edges

    return OUT


# %% WaveBreak break_steep


def calculate_Wavebreak_steep(Input):
    BREAK_STEEP = {}

    for ck, cd in DATA_SEC.items():

        break_steep_bool_list = []
        lamda_list = []

        for HS, TP in zip(cd[COLNAMES["Hs"]], cd[COLNAMES["Tp"]]):
            omega = 1 / TP * 2 * np.pi

            k = gl.k_aus_omega(omega, Input["d"])
            lamda = 2 * np.pi / k

            break_steep_bool = HS / lamda > Input["steep_crit"]

            break_steep_bool_list.append(break_steep_bool)
            lamda_list.append(lamda)

        break_steep_bool_list = pd.Series(
            break_steep_bool_list, name="bool_break", index=cd.index)
        lamda_list = pd.Series(
            lamda_list, name="lamda", index=cd.index)
        BREAK_STEEP[ck] = pd.concat([break_steep_bool_list, lamda_list], axis=1)

    return BREAK_STEEP


# %% Validation
def calc_Validation(Data_Out, Input, Data):
    def initiate_Validation_data(Data, Data_Out, Input):
        # Tables
        isData_VMHS = Data_Out["Tables"]["VMHS"]["isData"]
        isData_VMTP = Data_Out["Tables"]["VMTP"]["isData"]

        VMHS = Data_Out["Tables"]["VMHS"]["table_content"]
        VMTP = Data_Out["Tables"]["VMTP"]["table_content"]

        Count_Table = Data_Out["Tables"]["count"]

        Index_Table = Count_Table.copy()
        isData_Table_comb = Count_Table.copy()

        Hs = []
        Tp = []
        Index = []
        index = 0

        for col in isData_Table_comb.columns[1:]:
            row = 0
            for _ in isData_Table_comb.loc[:, col]:
                if isData_VMHS.loc[row, col] & isData_VMTP.loc[row, col]:

                    Hs.append(VMHS.loc[row, col])
                    Tp.append(VMTP.loc[row, col])
                    Index.append(index)
                    Index_Table.loc[row, col] = index
                    isData_Table_comb[row, col] = True
                    index = index + 1
                else:
                    Index_Table.loc[row, col] = float('nan')
                    isData_Table_comb[row, col] = False

                row = row + 1

        Hs = pd.Series(Hs, name='Hs')
        Tp = pd.Series(Tp, name='Tp')
        Point_id = pd.Series(Index, name='id')

        gamma = pd.Series(np.ones((len(Point_id))) * INPUT["Validation"]["gamma"])
        gamma.index = Hs.index
        gamma.name = 'gamma'

        df_table = pd.concat((Point_id, Hs, Tp, gamma), axis=1)

        # Points

        Vm_VMHS_lowest = min(
            [VM[0] for _, VM in Data_Out["Tables"]["VMHS"]["VM_Zone"].items()])
        Vm_VMTP_lowest = min(
            [VM[0] for _, VM in Data_Out["Tables"]["VMTP"]["VM_Zone"].items()])
        Vm_VMHS_highest = max(
            [VM[1] for _, VM in Data_Out["Tables"]["VMHS"]["VM_Zone"].items()])
        Vm_VMTP_highest = max(
            [VM[1] for _, VM in Data_Out["Tables"]["VMTP"]["VM_Zone"].items()])

        VM_lowest_inner = max(Vm_VMHS_lowest, Vm_VMTP_lowest)
        VM_highest_inner = min(Vm_VMHS_highest, Vm_VMTP_highest)

        Vm_VMHS_low = [VM[0]
                       for _, VM in Data_Out["Tables"]["VMHS"]["VM_Zone"].items()]
        Vm_VMTP_low = [VM[0]
                       for _, VM in Data_Out["Tables"]["VMTP"]["VM_Zone"].items()]

        VM_lowest_inner_local = [max(Vm_VMHS_low[i], Vm_VMTP_low[i])
                                 for i in range(len(Vm_VMHS_low))]

        Vm_VMHS_high = [VM[1]
                        for _, VM in Data_Out["Tables"]["VMHS"]["VM_Zone"].items()]
        Vm_VMTP_high = [VM[1]
                        for _, VM in Data_Out["Tables"]["VMTP"]["VM_Zone"].items()]

        VM_highest_inner_local = [min(Vm_VMHS_high[i], Vm_VMTP_high[i])
                                  for i in range(len(Vm_VMHS_high))]

        DATA_VAL = gl.filter_dataframe(
            Data, [COLNAMES["Vm"]], [VM_lowest_inner], [VM_highest_inner])

        Hs = DATA_VAL[COLNAMES['Hs']]
        Hs.name = 'Hs'
        Tp = DATA_VAL[COLNAMES['Tp']]
        Tp.name = 'Tp'
        Vm = DATA_VAL[COLNAMES['Vm']]
        Vm.name = 'Vm'
        angle = DATA_VAL[COLNAMES['angle']]
        angle.name = 'angle'

        angle.index = Hs.index

        Point_id = pd.Series(np.arange(len(DATA_VAL)), name='Index')
        Point_id.index = Hs.index
        Point_id.name = 'id'

        gamma = pd.Series(np.ones((len(DATA_VAL))) * INPUT["Validation"]["gamma"])
        gamma.index = Hs.index
        gamma.name = 'gamma'

        df_points = pd.concat((Point_id, angle, Hs, Tp, gamma, Vm), axis=1)

        JBOOST = {"set_table": {}}

        JBOOST["set_table"]["lims"] = {}
        JBOOST["set_table"]["lims"]["global_inner"] = [
            VM_lowest_inner, VM_highest_inner]
        JBOOST["set_table"]["lims"]["local_inner"] = [
            VM_lowest_inner_local, VM_highest_inner_local]
        JBOOST["set_table"]["table"] = df_table
        JBOOST["Vm_edges"] = Data_Out["Tables"]["Vm_edges"]

        JBOOST["set_points"] = {}
        JBOOST["set_points"]["table"] = df_points

        JBOOST["set_table"]["index_table"] = Index_Table
        JBOOST["set_table"]["count_table"] = Count_Table
        JBOOST["DATA"] = DATA_VAL
        return JBOOST

    def import_JBOOST(path):
        lookup = 'Result Hindcast'

        ID = []
        NODE = []
        IDLING = []
        PRODUCTION = []

        with open(path) as myFile:
            for num, line in enumerate(myFile, 1):
                if lookup in line:
                    line_num = num
                    break

            for num_2, line_2 in enumerate(myFile, line_num):
                if (num_2 > line_num + 1) & (line_2 != '\n'):
                    out = line_2.strip().split("\t")
                    ID.append(int(out[0]))
                    NODE.append(int(out[1]))
                    IDLING.append(float(out[2]))
                    PRODUCTION.append(float(out[3]))

        df = pd.DataFrame()
        ID_series = pd.Series(ID)
        Node_series = pd.Series(NODE)
        df['IDLING'] = IDLING
        df['PRODUCTION'] = PRODUCTION

        Nodes = Node_series.unique()

        JBOOST_OUT = {}
        for node in Nodes:
            temp = df[Node_series == node]
            temp.index = ID_series[Node_series == node]
            JBOOST_OUT['node ' + str(node)] = temp

        return JBOOST_OUT

    def calculate_DEL(Data_Out, Input):
        global INFO_LOG
        skal_time = (Input["design_life"] * 365.25 * 24)

        keys = [key for key in Data_Out["set_table"]["table"].keys(
        ) if 'PRODUCTION' in key or 'IDLING' in key]

        exp = create_dictionaries(segments)

        Data_Out["set_points"]["segments"] = {}
        Data_Out["set_points"]["DEL"] = {}
        Data_Out["set_table"]["DEL"] = {}
        Data_Out["Compare"] = {}
        Data_Out["set_points"]["DEL"]["added"] = pd.DataFrame(
            columns=keys, index=exp.keys())
        Data_Out["set_table"]["DEL"]["added"] = pd.DataFrame(
            columns=keys, index=exp.keys())
        Data_Out["Compare"]["added"] = pd.DataFrame(
            columns=keys, index=exp.keys())

        i = 0
        for ck, cd in EXP.items():
            Data_Out["set_points"]["segments"][ck] = gl.filter_dataframe(
                Data_Out["set_points"]["table"], ["angle"], [segments_mod[i][0]], [segments_mod[i][1]])
            i = i + 1

        # table vm-section vise

        DEL_Section_Table = {}
        col_num = 0
        VM_section = Data_Out["set_table"]["count_table"].iloc[:, 0]

        for col_name in Data_Out["set_table"]["count_table"].columns[1:]:
            DEL_Section_Table[col_name] = {}
            key_num = 0

            for key in keys:
                SUM_LIST = []
                TEMP_LIST = []
                for idx, cell in Data_Out["set_table"]["index_table"][col_name].items():
                    if ~np.isnan(cell):
                        count = Data_Out["set_table"]["count_table"].loc[idx, col_name]
                        indx_Force = int(Data_Out["set_table"]["index_table"].loc[idx, col_name])
                        Force = Data_Out["set_table"]["table"].loc[indx_Force, key]

                        temp = (Force ** Input["SN_slope"] * Input["N_ref"]) / skal_time * count

                        Force_vm_vise = (temp / Input["N_ref"]) ** (1 / Input["SN_slope"])

                    else:
                        Force_vm_vise = float('nan')
                        temp = float('nan')

                    SUM_LIST.append(Force_vm_vise)
                    TEMP_LIST.append(temp)
                DEL_Section_Table[col_name][key] = pd.Series(
                    SUM_LIST, index=VM_section)

                Data_Out["set_table"]["DEL"]["added"].loc[col_name, key] = (np.nansum(TEMP_LIST) / Input["N_ref"]) ** (
                        1 / Input["SN_slope"])

                key_num = key_num + 1

            col_num = col_num + 1

        # points vm-section vise
        num_sec = 0
        Vm_table_lims = Data_Out["set_table"]["lims"]["local_inner"]
        Vm_table_edges = Data_Out["Vm_edges"]
        DEL_Section_Points = {}
        DEL_Compare = {}

        MAX_DEL = {key: int(0) for key in keys}
        for _ in Data_Out["set_table"]["count_table"].columns[1:]:

            col_name = Data_Out["set_table"]["count_table"].columns[num_sec + 1]
            DEL_Section_Points[col_name] = {}
            DEL_Compare[col_name] = {}

            Data_segment = Data_Out["set_points"]["segments"][col_name]

            for key in keys:
                SUM_LIST = []

                for i_edge in range(len(Vm_table_edges) - 1):

                    if (Vm_table_edges[i_edge] >= Vm_table_lims[0][num_sec]) & (
                            Vm_table_edges[i_edge + 1] <= Vm_table_lims[1][num_sec]):

                        points_sec_vm = gl.filter_dataframe(
                            Data_segment, ['Vm'], [Vm_table_edges[i_edge]], [Vm_table_edges[i_edge + 1]])

                        Force = points_sec_vm[key]

                        temp = (Force ** Input["SN_slope"] * Input["N_ref"]) / skal_time

                        Force_vm_vise = (sum(temp) / Input["N_ref"]) ** (1 / Input["SN_slope"])
                        if Force_vm_vise == 0:
                            Force_vm_vise = float('nan')

                    else:
                        Force_vm_vise = float('nan')

                    SUM_LIST.append(Force_vm_vise)

                points_sec = Data_segment[key]

                GLOBAL_SUM = (sum((points_sec ** Input["SN_slope"] * Input["N_ref"]) / skal_time) / Input["N_ref"]) ** (
                        1 / Input["SN_slope"])

                MAX_TEMP = max(np.nanmax(SUM_LIST), np.nanmax(
                    DEL_Section_Table[col_name][key]))
                if MAX_TEMP > MAX_DEL[key]:
                    MAX_DEL[key] = MAX_TEMP

                DEL_Section_Points[col_name][key] = pd.Series(
                    SUM_LIST, index=VM_section)

                Data_Out["set_points"]["DEL"]["added"].loc[col_name, key] = GLOBAL_SUM

                COMPARE_LIST = 100 * DEL_Section_Table[col_name][key] / GLOBAL_SUM
                DEL_Compare[col_name][key] = pd.Series(
                    COMPARE_LIST, index=VM_section)

                Data_Out["Compare"]["added"].loc[col_name, key] = 100 * np.nansum(
                    DEL_Section_Table[col_name][key]) / np.nansum(SUM_LIST)
            num_sec = num_sec + 1

        Data_Out["set_points"]["DEL"]["vm-vise"] = DEL_Section_Points
        Data_Out["set_table"]["DEL"]["vm-vise"] = DEL_Section_Table
        Data_Out["Compare"]["vm-vise"] = DEL_Compare
        Data_Out["Max_DEL"] = MAX_DEL
        return

    path_valid = path_out + 'Validiation/'
    if not os.path.exists(path_valid):
        os.makedirs(path_valid)

    Data_Out["Validation"] = initiate_Validation_data(
        Data, Data_Out, Input)

    working_dir = path_main + "\\JBOOST"

    if Input["Validation"]["calc_DEL_base"]:

        # Modifing the project.lua file
        print("    updating variables in 'proj.lua'...")

        lua_file_path = r".\JBOOST\proj.lua"
        proj_var = {
            'design_life': Input["Validation"]['design_life'],
            'N_ref': Input["Validation"]['N_ref'],  # For strings, include the quotes
            'SN_slope': Input["Validation"]['SN_slope'],
            'f_0': Input["Validation"]['f_0'],

        }

        modify_lua_variables(lua_file_path, proj_var)

        # calculates the scatterpoints for the suplied dataset,
        # exports DEL_base files with JBOOST data and a lookup table
        print('    Mode: calculate full dataset')

        # calculate JBOOST input File for DATA
        print("     exporting JBOOST input Files of Database")
        DataFrame_to_JBOST_HsTp(Data_Out["Validation"]["set_points"]["table"], path_out + "Validiation/DEL_base.txt")

        # Copy JBOOST Input file to Working dir of JBOOST
        shutil.copy(path_out + "Validiation/DEL_base.txt",
                    ".//JBOOST//wave.lua")

        print("     run JBOOST for hindcast data (this might take a long time)")

        # Run JBOOST
        subprocess.check_call(['JBOOST.exe', 'proj.lua'],
                              cwd=working_dir, shell=True)

        # Copy result of JBOOST to output path
        shutil.copy(r".\JBOOST\Results_JBOOST_Text\JBOOST.out",
                    path_out + "Validiation/DEL_base.out")
        print("     import JBOOST outfiles of hindcast data")
        # import points of Database
        temp_points = import_JBOOST(path_out + "Validiation/DEL_base.out")

        for ck, cd in temp_points.items():
            temp_points[ck].index = Data_Out["Validation"]["set_points"]["table"]["id"].index

        print(f"     save DEL_base lookup table at {path_valid + 'DEL_base.txt'}")
        # crate lookuptable for Database
        Series_to_txt(Data_Out["Validation"]["set_points"]["table"]["id"], path_valid + 'DEL_base.txt')

    else:
        # reading of DEL_base files and integrating them in the calculated data,
        # only the data from the run is beeing used
        print(f"    load DEL Database from {Input['Validation']['path_DEL_base']}")
        temp_points = import_JBOOST(path_main + '\\' + Input["Validation"]["path_DEL_base"])
        TIME = []
        ID = []
        print(f"    load DEL database lookup from {Input['Validation']['path_DEL_base_lookup']}")
        with open(path_main + '\\' + Input["Validation"]["path_DEL_base_lookup"]) as data:
            for num, line in enumerate(data):
                if line != '\n':
                    out = line.strip().split("\t")
                    TIME.append(out[0])
                    ID.append(int(out[1]))

        lookup = pd.Series(TIME, index=ID, name='Datetime')
        lookup = pd.to_datetime(lookup)
        print("    combine DEL Database with lookup")
        for ck, cd in temp_points.items():

            temp = pd.merge(cd, lookup, left_index=True, right_index=True)

            if len(lookup) - len(cd):
                print("    WARNING, size difference of lookup and DEL_base files, sure they are from the same run?")

            temp_points[ck] = temp
            temp_points[ck].index = temp_points[ck]["Datetime"]
            temp_points[ck] = temp_points[ck].drop('Datetime', axis=1)

        # check timeframes
        if (Data.index[0] < lookup.iloc[0]) or (Data.index[-1] > lookup.iloc[-1]):
            print(
                "\t WARNING, the selected timeframe is outside of the DEL database! "
                "errors will occur and data is excluded from the calculation of the hindcast DELs"
                ", making the validation invalid\n \t" + f"Database timeframe: {lookup.iloc[0]} to {lookup.iloc[-1]}"
                + "\n \t" + f"loaded data timeframe: {DATA.index[0]} to {DATA.index[-1]}")

    # calculate condesed DELs

    # calculate JBOOST input File for DATA
    print("    exporting JBOOST input files of condensed data")
    DataFrame_to_JBOST_HsTp(Data_Out["Validation"]["set_table"]["table"], path_valid + 'condensed.txt')

    # Copy JBOOST Input file to Working dir of JBOOST
    shutil.copy(path_out + "Validiation/condensed.txt",
                ".//JBOOST//wave.lua")

    print("    run JBOOST for condensed data")

    # Run JBOOST
    subprocess.check_call(['JBOOST.exe', 'proj.lua'],
                          cwd=working_dir, shell=True)

    # Copy result of JBOOST to output path
    shutil.copy(r".\JBOOST\Results_JBOOST_Text\JBOOST.out",
                path_out + "Validiation/condensed.out")
    print("    import JBOOST outfiles of condensed data")
    # import condensed data
    temp_table = import_JBOOST(path_out + "Validiation/condensed.out")

    for ck, cd in temp_table.items():
        cd = cd.rename(columns={'IDLING': 'IDLING ' +
                                          ck, 'PRODUCTION': 'PRODUCTION ' + ck})

        Data_Out["Validation"]["set_table"]["table"] = pd.concat(
            [Data_Out["Validation"]["set_table"]["table"], cd], axis=1)

    for ck, cd in temp_points.items():
        cd = cd.rename(columns={'IDLING': 'IDLING ' +
                                          ck, 'PRODUCTION': 'PRODUCTION ' + ck})

        Data_Out["Validation"]["set_points"]["table"] = (
            Data_Out["Validation"]["set_points"]["table"].merge(cd, left_index=True, right_index=True, how='inner'))

    print("    determine cumulated DEL's")

    calculate_DEL(Data_Out["Validation"], Input["Validation"])

    return


# %% Angle_Seperation
def calculate_AngleDeviation(Data_Full, Input):
    #pointdata
    ANGLE_ORIG = Data_Full[COLNAMES['angle']]
    ANGLE_COMPARE = Data_Full[Input["angle_compare"]]
    HS = Data_Full[COLNAMES["Hs"]]
    df_angles = pd.merge(ANGLE_ORIG, ANGLE_COMPARE, left_index=True, right_index=True)
    bool_HS_null = HS <= 0

    PITAU = 359 + 180  # for readablility
    DIFF_ANG = (ANGLE_ORIG - ANGLE_COMPARE + PITAU) % 359 - 180
    DIFF_ANG_ABS = abs(DIFF_ANG)

    df_angles["Diff_angles"] = DIFF_ANG
    df_angles["Diff_angles_abs"] = DIFF_ANG_ABS
    df_angles["bool_Hs_null"] = bool_HS_null

    ANGLE_SEPERATION = {"points": df_angles}

    N = 100
    grid = np.linspace(0, 360, N)
    df = gl.grid_pointcloud_in_x(ANGLE_ORIG, DIFF_ANG_ABS, grid)

    ANGLE_SEPERATION["Angle_mean"] = df

    return ANGLE_SEPERATION

# %% Extreme Value Analysis
def calc_Extreme_Value(x, Input):

    def gumbel_F(x, beta, mu):
        F = np.exp(-np.exp(-(x - mu) / beta))
        return F

    def gumbel_F_inv(F, beta, mu):
        x = - beta * np.log(-1 * np.log(F)) + mu
        return x

    def gumbel_f(x, beta, mu):
        f = 1 / beta * np.exp(-1 / beta * (x - mu)) * np.exp(-1 * np.exp(-1 / beta * (x - mu)))
        return f

    def QQ_Intervall(N, itter):
        P = np.empty((N, itter))
        X = np.empty((N, itter))

        for i in range(itter):
            P_samp = [random.uniform(0, 1) for i in range(N)]
            P_samp.sort()

            x_samp = gumbel_F_inv(P_samp, beta, mu)
            P[:, i] = P_samp
            X[:, i] = x_samp

        perc_up = np.percentile(X, Input["perc_up"], axis=1)
        perc_middle = np.percentile(X, 50, axis=1)
        perc_down = np.percentile(X, Input["perc_down"], axis=1)
        std_dev = np.std(X, axis=1)
        return perc_down, perc_middle, perc_up, std_dev

    def Hs_to_T_R(Hs):
        F = gumbel_F(Hs, beta, mu)
        T_R = [1 / ((1 - F_curr) * freq_samp) if F_curr != 1 else float("nan") for F_curr in F]

        return T_R
    freq_samp = Input["freq_samp"]

    # maximal values in elvaluation window findwn
    years = np.unique(x.index.year)
    window = pd.Timedelta(Input["time_window_offeset"] * 365, "d")

    years_newyear = [datetime.date(year, 1, 1) for year in years]

    years_start = [new_year + window for new_year in years_newyear]

    x_max = pd.Series()

    for i in range(len(years_start) - 1):
        x_section = x.loc[years_start[i]:years_start[i + 1]]

        x_max_temp = max(x_section)
        indx_x_max = np.array(x_section).argmax()
        time_x_max = x_section.index[indx_x_max]

        x_max.at[time_x_max] = x_max_temp

    x_sortet = np.array(x_max.sort_values(ascending=True))
    N = len(x_sortet)

    #gumbel coefficents
    std = np.std(x_max)
    beta = std * np.sqrt(6) / np.pi
    gamma = 0.57721566490153286060651209008240243104215933593992
    mu = np.mean(x_max) - gamma * beta


    # grid theorie (gumbel)
    x_grid = np.linspace(-(max(x_max) - min(x_max)) * 0.2 + min(x_max),
                          (max(x_max) - min(x_max)) * 0.2 + max(x_max), 100)

    F_theroie = gumbel_F(x_grid, beta, mu)
    f_theroie = gumbel_f(x_grid, beta, mu)

    # real
    F_real = [(n + 0.5) / N for n in range(N)]
    # theoretische erwartungswerte nach gumbelverteilung
    x_theorie = gumbel_F_inv(F_real, beta, mu)

    perc_down, perc_middle, perc_up, std_dev = QQ_Intervall(len(x_sortet), Input["itter_deviations"])

    T_R_real = [1 / ((1 - F_curr) * freq_samp) if F_curr != 1 else float("nan") for F_curr in F_real]

    T_R_gumbel = Hs_to_T_R(perc_middle)


    #single Returnperiods
    quantile_return_single = [(1 - 1 / (T_retrun_temp * freq_samp)) for T_retrun_temp in Input["T_Return_single"]]

    x_T_return_single = gumbel_F_inv(quantile_return_single, beta, mu)

    points = pd.DataFrame({
        "x_max": x_max,
        "x_sortet": x_sortet,
        "F_real": F_real,
        "perc_up": perc_up,
        "perc_down": perc_down,
        "perc_middle": perc_middle,
        "T_R_real": T_R_real,
        "T_R_gumbel": T_R_gumbel,
        "x_theorie": x_theorie
    })
    grid = pd.DataFrame({
        "F_theorie": F_theroie,
        "f_therie": f_theroie,
        "x_grid": x_grid,
    })

    EXTREME_VALUES = {"points": points, "grid": grid, "years": {"newyear": years_newyear, "start_window": years_start}, "x_T_return_single": x_T_return_single}

    return EXTREME_VALUES
# </editor-fold>
# <editor-fold desc="Main_LOG">

# %% Main_Log
INFO_LOG = str()

print("\n***Starting " + f"{script_name}" +
      ", this might take a few minutes***\n")

# %% Main_UserInput
print(f"reading Inputfile ({path_in})...")

INPUT = read_input_txt(path_in)
INPUT_SELECT = INPUT["SELECT"]
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

if args.o is None:
    if INPUT['SELECT']['dir_name'] is None:
        path_out = './out/' + 'HindCast_' + timestamp + '/'
    else:
        path_out = INPUT['SELECT']['path_out'] + \
                   INPUT['SELECT']['dir_name'] + '/'
else:
    path_out = args.o + '/'

INFO_LOG += f"Path_out = {path_out}" + "\n \n"

print(f"Path_out = {path_out}")

if not os.path.exists(path_out):
    os.makedirs(path_out)

shutil.copy(path_in, path_out + 'Input.txt')
# </editor-fold>

# <editor-fold desc="Main_Segments">
# %% Main_Segments
print("calculating Segments...")

segments, segments_mod = angles(INPUT["SELECT"]["mode_angle"], INPUT["SELECT"]["N_angle"],
                                INPUT["SELECT"]["angle_start"], INPUT["SELECT"]["width_angle"])

if INPUT["SELECT"]["include_360"]:
    segments.insert(0, [0, 360])
    segments_mod.insert(0, [0, 360])

INFO_LOG += 'angles:   ' + \
            (f'mode: {INPUT["SELECT"]["mode_angle"]}, number segments: {INPUT["SELECT"]["N_angle"]}, '
             f'start angle: {INPUT["SELECT"]["width_angle"]}'
             f', width: (if mode = section): {INPUT["SELECT"]["width_angle"]}') + '\n'

# </editor-fold>

# <editor-fold desc="Main_DataRead/Filt">
# %% Main_DataRead/Filt
DataEval = {"timestamps": {}, "NAN": {}}

if INPUT["SELECT"]["DATA"] == "MetOcean":
    print("reading MetOcean Data...")
    DATA_RAW = DATA_metocean_read(INPUT["DATA_METOCEAN"])

if INPUT["SELECT"]["DATA"] == "APG_mer":
    print("reading APG_mer Data...")

    DATA_RAW = DATA_ABP_mer_read(INPUT["APG_mer"])

if INPUT["SELECT"]["DATA"] == "General":
    print("reading General Data...")

    DATA_RAW = DATA_general_read(INPUT["General"])

# Colnames
COLNAMES = {}
try:
    temp = DATA_RAW[INPUT_SELECT["col_name_Vm"]]
    COLNAMES['Vm'] = temp.name
except KeyError:
    print(
        f"    CRITICAL: col_name_Vm = '{INPUT_SELECT['col_name_Vm']}' not found in Data")

try:
    temp = DATA_RAW[INPUT_SELECT["col_name_Hs"]]
    COLNAMES['Hs'] = temp.name
except KeyError:
    print(
        f"    CRITICAL: col_name_Hs = '{INPUT_SELECT['col_name_Hs']}' not found in Data")

try:
    temp = DATA_RAW[INPUT_SELECT["col_name_Tp"]]
    COLNAMES['Tp'] = temp.name
except KeyError:
    print(
        f"    CRITICAL: col_name_Tp = '{INPUT_SELECT['col_name_Tp']}' not found in Data")

try:
    temp = DATA_RAW[INPUT_SELECT["col_name_angle"]]
    COLNAMES['angle'] = temp.name
except KeyError:
    print(
        f"    CRITICAL: col_name_angle = '{INPUT_SELECT['col_name_angle']}' not found in Data")

del temp

# Filt
# filter Tp_Zero

if INPUT_SELECT["exclude_Tp_zero"]:
    DATA_RAW_TP0 = DATA_RAW.loc[~(
        (DATA_RAW[INPUT_SELECT["col_name_Tp"]] == 0))]
else:
    DATA_RAW_TP0 = DATA_RAW

# Filter Datetimesection
if INPUT["SELECT"]["timeframe"]:
    try:
        DATA_FULL = DATA_RAW_TP0.loc[(DATA_RAW_TP0.index > INPUT["SELECT"]['datetime_start']) & (
                DATA_RAW_TP0.index <= INPUT["SELECT"]['datetime_end'])]
        INFO_LOG += (f"evaluated timeframe: [{INPUT['SELECT']['datetime_start']} - {INPUT['SELECT']['datetime_end']}]"
                     + "\n \n")
    except TypeError as e:
        print(
            f"WARNING: cant read {INPUT['SELECT']['datetime_start']} or {INPUT['SELECT']['datetime_end']}"
            f", invalid format ({e}), no filtering applied")

else:

    DATA_FULL = DATA_RAW_TP0

TIMEFRAME = [DATA_FULL.index[0], DATA_FULL.index[-1]]

CALCLIMS = {'Vm': np.array(INPUT_SELECT["Vm_lim"], dtype=np.float64)}

if np.isnan(CALCLIMS['Vm'][1]):
    CALCLIMS['Vm'][1] = max(DATA_FULL[COLNAMES['Vm']])

CALCLIMS['Hs'] = np.array(INPUT_SELECT["Hs_lim"], dtype=np.float64)
if np.isnan(CALCLIMS['Hs'][1]):
    CALCLIMS['Hs'][1] = max(DATA_FULL[COLNAMES['Hs']])

CALCLIMS['Tp'] = np.array(INPUT_SELECT["Tp_lim"], dtype=np.float64)
if np.isnan(CALCLIMS['Tp'][1]):
    CALCLIMS['Tp'][1] = max(DATA_FULL[COLNAMES['Tp']])

# Filter by Lims
DATA_FULL_FILT = gl.filter_dataframe(DATA_FULL, [COLNAMES['Vm'], COLNAMES['Hs'], COLNAMES['Tp']], [
    CALCLIMS["Vm"][0], CALCLIMS["Hs"][0], CALCLIMS["Tp"][0]], [CALCLIMS["Vm"][1], CALCLIMS["Hs"][1], CALCLIMS["Tp"][1]])

DATA = pd.concat([DATA_FULL_FILT[[COLNAMES["angle"]]], DATA_FULL_FILT[[COLNAMES["Vm"]]], DATA_FULL_FILT[[
    COLNAMES["Hs"]]], DATA_FULL_FILT[[COLNAMES["Tp"]]]], axis=1)

# delete Empty segments and segment names
EXP = create_dictionaries(segments)
j = 0
segments_neu = []
segments_mod_neu = []
for _ in EXP.items():

    if len(gl.filter_dataframe(DATA, [INPUT_SELECT["col_name_angle"]], [segments_mod[j][0]],
                               [segments_mod[j][1]])) != 0:

        segments_neu.append(segments[j])
        segments_mod_neu.append(segments_mod[j])
    else:
        print(
            f"    Anglesegment f{segments[j]} is excluded from calculations, plots and outputs, no data found ")
        INFO_LOG += f"Anglesegment f{segments[j]} is excluded from calculations, plots and outputs, no data found "

    j = j + 1

segments = segments_neu
segments_mod = segments_mod_neu

del segments_neu, segments_mod_neu

# %% Main_DevideInSections

DATA_SEC = create_dictionaries(segments)

j = 0
for ck_DATA, _ in DATA_SEC.items():
    DATA_SEC[ck_DATA] = gl.filter_dataframe(
        DATA, [COLNAMES["angle"]], [segments_mod[j][0]], [segments_mod[j][1]])
    j = j + 1

# %% Data evaluation

DataEval["Sizes"] = {}
DataEval["Sizes"]["all"] = len(DATA_RAW)
DataEval["Sizes"]["tp_0_filt"] = len(DATA_RAW) - len(DATA_RAW_TP0)
DataEval["Sizes"]["timeframe_filt"] = len(DATA_RAW_TP0) - len(DATA_FULL)
DataEval["Sizes"]["parameter_filt"] = len(DATA_FULL) - len(DATA_FULL_FILT)
DataEval["Sizes"]["used"] = len(DATA)
DataEval["Sizes"]["used_segments"] = {}

for ck_DATA, cd_DATA in DATA_SEC.items():
    DataEval["Sizes"]["used_segments"][ck_DATA] = len(cd_DATA)

INFO_LOG += "Data Basis: \n\n"

for DictNames, Dicts in DataEval.items():
    INFO_LOG += f"{DictNames}: " + "\n"
    for names, values in Dicts.items():
        INFO_LOG += f"\t{names}: {values}" + "\n"

hc_plt.setup(DATA_SEC, segments, path_main, DATA, INPUT_SELECT, COLNAMES, Colors, path_out)

# </editor-fold>

# <editor-fold desc="Main_Calculate">

# %% Main_Calculate

if INPUT["SELECT"]["calc_VMHS"] | INPUT["SELECT"]["Validation"] | INPUT["SELECT"]["Tables"] | INPUT["SELECT"][
    "calc_VMTP"]:
    INFO_LOG += '\n \n \n' + \
                'calculation VMHS: (significant wave height over mean wind speed) \n'

    print("calculating VMHS...")

    DATA_OUT["VMHS"] = calc_VMHS(DATA, INPUT["VMHS"])

if INPUT["SELECT"]["calc_HSTP"] | INPUT["SELECT"]["Validation"] | INPUT["SELECT"]["Tables"] | INPUT["SELECT"][
    "calc_VMTP"]:
    INFO_LOG += '\n \n \n' + \
                'calculation HSTP: (peak period over significant wave height) \n'
    print("calculating HSTP...")
    DATA_OUT["HSTP"] = calc_HSTP(DATA, INPUT["HSTP"])

if INPUT["SELECT"]["calc_VMTP"] | INPUT["SELECT"]["Validation"] | INPUT["SELECT"]["Tables"]:
    INFO_LOG += '\n \n \n' + \
                'calculation VMTP\n'
    print("calculating VMTP...")
    DATA_OUT["VMTP"] = calc_VMTP(INPUT, DATA_OUT)

if INPUT["SELECT"]["Tables"] | INPUT["SELECT"]["Validation"]:
    print("calculating Tables...")

    DATA_OUT["Tables"] = calc_tables(DATA_OUT, INPUT)

if INPUT["SELECT"]["RosePlot"]:
    print("calculating RosePlot...")

    DATA_OUT["RosePlot"] = calc_RosePlot(DATA_FULL_FILT, INPUT["RosePlot"])

if INPUT["SELECT"]["calc_RWI"]:
    print("calculating RWI...")

    DATA_OUT["RWI"] = calculate_RWI(DATA, INPUT["RWI"])

if INPUT["SELECT"]["calc_WaveBreak_Steep"]:
    print("calculation WaveBreak_Steep...")

    DATA_OUT["WaveBreak_Steep"] = calculate_Wavebreak_steep(INPUT["WaveBreak_Steep"])

if INPUT["SELECT"]["Angle_Deviation"]:
    print("calculation Angle_Deviation...")

    DATA_OUT["Angle_Deviation"] = calculate_AngleDeviation(DATA_FULL, INPUT["Angle_Deviation"])

if INPUT["SELECT"]["Extreme_Analyis"]:
    print("calculation Extreme Value...")
    if INPUT["Extreme_Analyis"]["col_name_values"] == "Hs": INPUT["Extreme_Analyis"]["col_name_values"] = COLNAMES["Hs"]

    DATA_OUT["Extreme_Analyis"] = calc_Extreme_Value(DATA_FULL_FILT[INPUT["Extreme_Analyis"]["col_name_values"]], INPUT["Extreme_Analyis"])


# </editor-fold>

# <editor-fold desc="Main_plot">

# %% Main_plot

# global Settings

plt.rcParams["font.family"] = "Arial"
params = {'legend.fontsize': 6,
          'axes.labelsize': 8,
          'axes.titlesize': 8,
          'xtick.labelsize': 7,
          'ytick.labelsize': 7}
plt.rcParams.update(params)

if (len(INPUT["SELECT"]["plot_VMHS"]) > 0) & INPUT["SELECT"]["calc_VMHS"]:
    print("plotting VMHS...")

    LIMS = {'x_lims': INPUT["VMHS"]["plot_xlims"], 'y_lims': INPUT["VMHS"]["plot_ylims"]}

    if INPUT["VMHS"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Vm"][1]

    if INPUT["VMHS"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Hs"][1]

    plot_select = INPUT["SELECT"]["plot_VMHS"]

    INPUT["VMHS"]['col_name_x'] = COLNAMES["Vm"]
    INPUT["VMHS"]['col_name_y'] = COLNAMES["Hs"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled(DATA_OUT["VMHS"], INPUT["VMHS"], LIMS)

        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'VMHS_tiled_')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + f'VMHS_tiled_')

    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single(DATA_OUT["VMHS"], INPUT["VMHS"], LIMS, num_curr - 1)

        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + f'VMHS_single_sec_{pos_num_curr}')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + f'VMHS_single_sec_{pos_num_curr}')

if (len(INPUT["SELECT"]["plot_HSTP"]) > 0) & INPUT["SELECT"]["calc_HSTP"]:
    print("plotting HSTP...")

    LIMS = {'x_lims': INPUT["HSTP"]["plot_xlims"], 'y_lims': INPUT["HSTP"]["plot_ylims"]}

    if INPUT["HSTP"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Hs"][1]

    if INPUT["HSTP"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Tp"][1]

    plot_select = INPUT["SELECT"]["plot_HSTP"]

    INPUT["HSTP"]['col_name_x'] = COLNAMES["Hs"]
    INPUT["HSTP"]['col_name_y'] = COLNAMES["Tp"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled(DATA_OUT["HSTP"], INPUT["HSTP"], LIMS)

        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'HSTP_tiled_')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + f'HSTP_tiled_')

    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single(DATA_OUT["HSTP"], INPUT["HSTP"], LIMS, num_curr - 1)
            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'HSTP_single_sec_{pos_num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'HSTP_single_sec_{pos_num_curr}')

if (len(INPUT["SELECT"]["plot_VMTP"]) > 0) & INPUT["SELECT"]["calc_VMTP"]:

    print("plotting VMTP...")

    LIMS = {'x_lims': INPUT["VMTP"]["plot_xlims"], 'y_lims': INPUT["VMTP"]["plot_ylims"]}

    if INPUT["VMTP"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Vm"][1]

    if INPUT["VMTP"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Tp"][1]

    plot_select = INPUT["SELECT"]["plot_VMTP"]

    INPUT["VMTP"]['col_name_x'] = COLNAMES["Vm"]
    INPUT["VMTP"]['col_name_y'] = COLNAMES["Tp"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled(DATA_OUT["VMTP"], INPUT["VMTP"], LIMS)

        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'VMTP_tiled_')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + f'VMTP_tiled_')

    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single(DATA_OUT["VMTP"], INPUT["VMTP"], LIMS, num_curr - 1)

            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'VMTP_single_sec_{pos_num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'VMTP_single_sec_{pos_num_curr}')

if (len(INPUT["SELECT"]["plot_RWI"]) > 0) & INPUT["SELECT"]["calc_RWI"]:

    print("plotting RWI...")

    LIMS = {'x_lims': INPUT["RWI"]["plot_xlims"], 'y_lims': INPUT["RWI"]["plot_ylims"]}

    if INPUT["RWI"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Hs"][1]

    if INPUT["RWI"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Tp"][1]

    plot_select = INPUT["SELECT"]["plot_RWI"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled_RWI(DATA_OUT["RWI"], INPUT["RWI"], LIMS)
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'RWI_tiled')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + f'RWI_tiled')
    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single_RWI(DATA_OUT["RWI"], INPUT["RWI"], LIMS, num_curr - 1)
            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'RWI_single_sec_{pos_num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'RWI_single_sec_{pos_num_curr}')

if INPUT["SELECT"]["Tables"]:
    print("plotting Tables...")

    FIG = hc_plt.plot_tables(DATA_OUT["Tables"], INPUT)

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Tables')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Tables')

if INPUT["SELECT"]["RosePlot"]:
    print("plotting RosePlot...")

    FIG = hc_plt.plot_RosePlot(DATA_OUT["RosePlot"], INPUT["RosePlot"])

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'RosePlot')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'RosePlot')

if (len(INPUT["SELECT"]["plot_WaveBreak_Steep"]) > 0) & INPUT["SELECT"]["calc_WaveBreak_Steep"]:

    print("plotting WaveBreak break_steep...")
    LIMS = {'x_lims': INPUT["WaveBreak_Steep"]["plot_xlims"], 'y_lims': INPUT["WaveBreak_Steep"]["plot_ylims"]}

    if INPUT["WaveBreak_Steep"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Hs"][1]

    if INPUT["WaveBreak_Steep"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Tp"][1]

    plot_select = INPUT["SELECT"]["plot_WaveBreak_Steep"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled_break_steep(
            DATA_OUT["WaveBreak_Steep"], INPUT["WaveBreak_Steep"], LIMS)
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'WaveBreak_Steep_tiled')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + 'WaveBreak_Steep_tiled')
    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single_break_steep(
                DATA_OUT["WaveBreak_Steep"], INPUT["WaveBreak_Steep"], LIMS, num_curr - 1)
            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'WaveBreak_Steep_single_sec_{pos_num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'WaveBreak_Steep_single_sec_{pos_num_curr}')

if INPUT["SELECT"]["Data_eval"]:
    print("plotting histogram...")

    LIMS = {'x_lims': INPUT["Histogram"]["plot_xlims_Hs"], 'y_lims': INPUT["Histogram"]["plot_ylims_Hs"]}
    AXLABELS = {'x': INPUT["Histogram"]["name_x_axis_Hs"], 'y': INPUT["Histogram"]["name_y_axis_Hs"]}

    if INPUT["Histogram"]["plot_xlims_Hs"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Hs"][1]

    if INPUT["Histogram"]["name_x_axis_Hs"] is None:
        AXLABELS['x'] = COLNAMES["Hs"]
    if INPUT["Histogram"]["name_y_axis_Hs"] is None:
        AXLABELS['y'] = "Number of datapoints"

    print("    Hs")

    FIG = hc_plt.plot_histo(DATA[COLNAMES["Hs"]], AXLABELS, LIMS, INPUT["Histogram"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Histogram_Hs')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Histogram_Hs')

    LIMS = {'x_lims': INPUT["Histogram"]["plot_xlims_Tp"], 'y_lims': INPUT["Histogram"]["plot_ylims_Tp"]}
    AXLABELS = {'x': INPUT["Histogram"]["name_x_axis_Tp"], 'y': INPUT["Histogram"]["name_y_axis_Tp"]}

    if INPUT["Histogram"]["plot_xlims_Tp"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Tp"][1]

    if INPUT["Histogram"]["name_x_axis_Tp"] is None:
        AXLABELS['x'] = COLNAMES["Tp"]
    if INPUT["Histogram"]["name_y_axis_Tp"] is None:
        AXLABELS['y'] = "Number of datapoints"

    print("    Tp")

    FIG = hc_plt.plot_histo(DATA[COLNAMES["Tp"]], AXLABELS, LIMS, INPUT["Histogram"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Histogram_Tp')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Histogram_Tp')

    LIMS = {'x_lims': INPUT["Histogram"]["plot_xlims_Vm"], 'y_lims': INPUT["Histogram"]["plot_ylims_Vm"]}
    AXLABELS = {'x': INPUT["Histogram"]["name_x_axis_Vm"], 'y': INPUT["Histogram"]["name_y_axis_Vm"]}

    if INPUT["Histogram"]["plot_xlims_Vm"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Vm"][1]

    if INPUT["Histogram"]["name_x_axis_Vm"] is None:
        AXLABELS['x'] = COLNAMES["Vm"]
    if INPUT["Histogram"]["name_y_axis_Vm"] is None:
        AXLABELS['y'] = "Number of datapoints"

    pdf_name = 'Histogram_Vm'

    print("    Vm")

    FIG = hc_plt.plot_histo(DATA[COLNAMES["Vm"]], AXLABELS, LIMS, INPUT["Histogram"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Histogram_Vm')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Histogram_Vm')

    if INPUT["Histogram"]["col_name_free"] is not None:

        LIMS = {'x_lims': INPUT["Histogram"]["plot_xlims_free"], 'y_lims': INPUT["Histogram"]["plot_ylims_free"]}
        AXLABELS = {'x': INPUT["Histogram"]["name_x_axis_free"], 'y': INPUT["Histogram"]["name_y_axis_free"]}
        if INPUT["Histogram"]["plot_xlims_free"][1] is None:
            LIMS['x_lims'][1] = max(DATA[INPUT["Histogram"]["col_name_free"]])

        if INPUT["Histogram"]["name_x_axis_free"] is None:
            AXLABELS['x'] = DATA[INPUT["Histogram"]["col_name_free"]].name
        if INPUT["Histogram"]["name_y_axis_free"] is None:
            AXLABELS['y'] = "Number of datapoints"

        pdf_name = 'Histogram_free'

        print("    free")

        FIG = hc_plt.plot_histo(DATA[INPUT["Histogram"]["col_name_free"]], AXLABELS, LIMS, INPUT["Histogram"])
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'Histogram_free')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + 'Histogram_free')

    print("plotting timeseries...")

    LIMS = {'x_lims': INPUT["Timeseries"]["plot_xlims_Hs"], 'y_lims': INPUT["Timeseries"]["plot_ylims_Hs"]}
    AXLABELS = {'x': INPUT["Timeseries"]["name_x_axis_Hs"], 'y': INPUT["Timeseries"]["name_y_axis_Hs"]}

    if INPUT["Timeseries"]["plot_xlims_Hs"][0] == 0:
        LIMS["x_lims"][0] = TIMEFRAME[0]

    if INPUT["Timeseries"]["plot_xlims_Hs"][1] is None:
        LIMS["x_lims"][1] = TIMEFRAME[1]

    if INPUT["Timeseries"]["name_x_axis_Hs"] is None:
        AXLABELS['x'] = "Date"
    if INPUT["Timeseries"]["name_y_axis_Hs"] is None:
        AXLABELS['y'] = COLNAMES["Hs"]

    print("    Hs")

    FIG = hc_plt.plot_timeseries(DATA[COLNAMES["Hs"]], AXLABELS, LIMS, INPUT["Timeseries"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Timeseries_Hs')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Timeseries_Hs')
    LIMS = {'x_lims': INPUT["Timeseries"]["plot_xlims_Tp"], 'y_lims': INPUT["Timeseries"]["plot_ylims_Tp"]}
    AXLABELS = {'x': INPUT["Timeseries"]["name_x_axis_Tp"], 'y': INPUT["Timeseries"]["name_y_axis_Tp"]}

    if INPUT["Timeseries"]["plot_xlims_Tp"][0] == 0:
        LIMS["x_lims"][0] = TIMEFRAME[0]

    if INPUT["Timeseries"]["plot_xlims_Tp"][1] is None:
        LIMS["x_lims"][1] = TIMEFRAME[1]

    if INPUT["Timeseries"]["name_x_axis_Tp"] is None:
        AXLABELS['x'] = "Date"
    if INPUT["Histogram"]["name_y_axis_Tp"] is None:
        AXLABELS['y'] = COLNAMES["Tp"]

    print("    Tp")

    FIG = hc_plt.plot_timeseries(DATA[COLNAMES["Tp"]], AXLABELS, LIMS, INPUT["Timeseries"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Timeseries_Tp')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Timeseries_Tp')
    LIMS = {'x_lims': INPUT["Timeseries"]["plot_xlims_Vm"], 'y_lims': INPUT["Timeseries"]["plot_ylims_Vm"]}
    AXLABELS = {'x': INPUT["Timeseries"]["name_x_axis_Vm"], 'y': INPUT["Timeseries"]["name_y_axis_Vm"]}

    if INPUT["Timeseries"]["plot_xlims_Vm"][0] == 0:
        LIMS["x_lims"][0] = TIMEFRAME[0]

    if INPUT["Timeseries"]["plot_xlims_Vm"][1] is None:
        LIMS["x_lims"][1] = TIMEFRAME[1]

    if INPUT["Timeseries"]["name_x_axis_Vm"] is None:
        AXLABELS['x'] = "Date"
    if INPUT["Timeseries"]["name_y_axis_Vm"] is None:
        AXLABELS['y'] = COLNAMES["Vm"]

    print("    Vm")

    FIG = hc_plt.plot_timeseries(DATA[COLNAMES["Vm"]], AXLABELS, LIMS, INPUT["Timeseries"])
    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Timeseries_Vm')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Timeseries_Vm')

    if INPUT["Timeseries"]["col_name_free"] is not None:

        LIMS = {'x_lims': INPUT["Timeseries"]["plot_xlims_free"], 'y_lims': INPUT["Timeseries"]["plot_ylims_free"]}
        AXLABELS = {'x': INPUT["Timeseries"]["name_x_axis_free"], 'y': INPUT["Timeseries"]["name_y_axis_free"]}

        if INPUT["plot_xlims_free"][0] == 0:
            LIMS["x_lims"][0] = TIMEFRAME[0]

        if INPUT["plot_xlims_free"][1] is None:
            LIMS["x_lims"][1] = TIMEFRAME[1]

        if INPUT["Timeseries"]["name_x_axis_free"] is None:
            AXLABELS['x'] = "Date"
        if INPUT["Timeseries"]["name_y_axis_free"] is None:
            AXLABELS['y'] = DATA[INPUT["Histogram"]["col_name_free"]].name

        pdf_name = 'Timeseries_free.pdf'

        print("    free")

        FIG = hc_plt.plot_histo(DATA[INPUT["Timeseries"]["col_name_free"]], AXLABELS, LIMS, INPUT["Timeseries"])
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'Timeseries_free')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + 'Timeseries_free')

if INPUT["SELECT"]["Angle_Deviation"]:
    print("plotting Angle_Deviation...")

    LIMS = {'x_lims': INPUT["Angle_Deviation"]["plot_xlims"], 'y_lims': INPUT["Angle_Deviation"]["plot_ylims"]}
    AXLABELS = {'x': INPUT["Angle_Deviation"]["name_x_axis"], 'y': INPUT["Angle_Deviation"]["name_y_axis"]}

    if INPUT["Angle_Deviation"]["name_x_axis"] is None:
        AXLABELS['x'] = "Original angle [deg]"

    if INPUT["Histogram"]["name_y_axis_Hs"] is None:
        AXLABELS['y'] = "deviation to Compare angle [deg]"

    FIG = hc_plt.plot_CompAngle_points(DATA_OUT["Angle_Deviation"], AXLABELS, LIMS, INPUT["Angle_Deviation"])

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'CompareAngle_points')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'CompareAngle_points')

if INPUT["SELECT"]["plot_Extreme_Analyis"]:
    print("plotting Extreme_Analyis...")

    LIMS = {'x_lims': INPUT["Extreme_Analyis"]["plot_xlims_timeseries"], 'y_lims': INPUT["Extreme_Analyis"]["plot_ylims_timeseries"]}
    AXLABELS = {'x': INPUT["Extreme_Analyis"]["name_x_axis_timeseries"], 'y': INPUT["Extreme_Analyis"]["name_y_axis_timeseries"]}

    if INPUT["Extreme_Analyis"]["plot_xlims_timeseries"][0] is None:
        LIMS["x_lims"][0] = DATA[COLNAMES["Hs"]].index[0]

    if INPUT["Extreme_Analyis"]["plot_ylims_timeseries"][1] is None:
        LIMS["x_lims"][1] = DATA[COLNAMES["Hs"]].index[-1]

    if INPUT["Extreme_Analyis"]["name_x_axis_timeseries"] is None:
        AXLABELS['x'] = "date"

    if INPUT["Extreme_Analyis"]["name_y_axis_timeseries"] is None:
        AXLABELS['y'] = INPUT["Extreme_Analyis"]["col_name_values"]

    FIG = hc_plt.plot_Extreme_timeseries(DATA[INPUT["Extreme_Analyis"]["col_name_values"]], DATA_OUT["Extreme_Analyis"], AXLABELS, LIMS, INPUT["Extreme_Analyis"])

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Extreme_Analyis_timeseries')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Extreme_Analyis_timeseries')


    LIMS = {'x_lims': INPUT["Extreme_Analyis"]["plot_xlims_qq"], 'y_lims': INPUT["Extreme_Analyis"]["plot_ylims_qq"]}
    AXLABELS = {'x': INPUT["Extreme_Analyis"]["name_x_axis_qq"], 'y': INPUT["Extreme_Analyis"]["name_y_axis_qq"]}

    if INPUT["Extreme_Analyis"]["name_x_axis_qq"] is None:
        AXLABELS['x'] = "theoretical values (gumbel)"

    if INPUT["Extreme_Analyis"]["name_y_axis_qq"] is None:
        AXLABELS['y'] = "real values"

    FIG = hc_plt.plot_Extreme_qq(DATA_OUT["Extreme_Analyis"], AXLABELS, LIMS, INPUT["Extreme_Analyis"])

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Extreme_Analyis_qq')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Extreme_Analyis_qq')


    LIMS = {'x_lims': INPUT["Extreme_Analyis"]["plot_xlims_T_Return"], 'y_lims': INPUT["Extreme_Analyis"]["plot_ylims_T_Return"]}
    AXLABELS = {'x': INPUT["Extreme_Analyis"]["name_x_axis_T_Return"], 'y': INPUT["Extreme_Analyis"]["name_y_axis_T_Return"]}

    if INPUT["Extreme_Analyis"]["name_x_axis_qq"] is None:
        AXLABELS['x'] = "Returnperiod in years"

    if INPUT["Extreme_Analyis"]["name_y_axis_qq"] is None:
        AXLABELS['y'] =INPUT["Extreme_Analyis"]["col_name_values"]

    FIG = hc_plt.plot_Extreme_T_Return(DATA_OUT["Extreme_Analyis"], AXLABELS, LIMS, INPUT["Extreme_Analyis"])

    if "pdf" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_pdf(FIG, path_out + 'Extreme_Analyis_T_Return')

    if "png" in INPUT_SELECT["plot_as"]:
        hc_plt.save_figs_as_png(FIG, path_out + 'Extreme_Analyis_T_Return')

#</editor-fold>

#<editor-fold desc="Main_Validation">

# %% Main_Validation

if INPUT["SELECT"]["Validation"]:
    print("conducting Validation...")
    calc_Validation(DATA_OUT, INPUT, DATA)

# Plot
if (len(INPUT["SELECT"]["plot_Validation_line"]) > 0) & INPUT["SELECT"]["Validation"]:
    print("plotting Validation lineplot")
    LIMS = {'x_lims': INPUT["Validation"]["plot_xlims"], 'y_lims': INPUT["Validation"]["plot_ylims"]}

    if INPUT["Validation"]["plot_xlims"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Vm"][1]

    y_max = max([DEL_Max for DEL_Max in DATA_OUT["Validation"]["Max_DEL"].values()])
    if INPUT["Validation"]["plot_ylims"][1] is None:
        LIMS['y_lims'][1] = y_max

    plot_select = INPUT["SELECT"]["plot_Validation_line"]

    if "tiled" in plot_select:
        print("    tiled plot")
        FIG = hc_plt.plot_tiled_DEL(
            DATA_OUT["Validation"], INPUT["Validation"], LIMS)
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'Validation_tiled')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + 'Validation_tiled')
    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single_DEL(
                DATA_OUT["Validation"], INPUT["Validation"], LIMS, num_curr - 1)

            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'Validation_single_sec_{num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'Validation_single_sec_{num_curr}')
if (len(INPUT["SELECT"]["plot_Validation_scatter"]) > 0) & INPUT["SELECT"]["Validation"]:
    print("plotting Validation scatterplot")

    LIMS = {'x_lims': INPUT["Validation"]["plot_xlims_scatter"], 'y_lims': INPUT["Validation"]["plot_ylims_scatter"]}
    plot_select = INPUT["SELECT"]["plot_Validation_scatter"]

    if INPUT["Validation"]["plot_xlims_scatter"][1] is None:
        LIMS['x_lims'][1] = CALCLIMS["Hs"][1]

    if INPUT["Validation"]["plot_ylims_scatter"][1] is None:
        LIMS['y_lims'][1] = CALCLIMS["Tp"][1]

    if "tiled" in plot_select:
        print("    tiled plot")

        FIG = hc_plt.plot_tiled_DEL_Scatter(
            DATA_OUT["Validation"], INPUT["Validation"], LIMS)
        if "pdf" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_pdf(FIG, path_out + 'Validation_scatter_tiled')

        if "png" in INPUT_SELECT["plot_as"]:
            hc_plt.save_figs_as_png(FIG, path_out + 'Validation_scatter_tiled')

    if "single" in plot_select:
        flag_rwi = [s == 'single' for s in plot_select]
        pos_num_plot = [pos + 1 for pos in np.where(flag_rwi)]

        for pos_num_curr in pos_num_plot[0]:
            num_curr = plot_select[pos_num_curr]

            print(f"    single plot segment number {num_curr}")
            FIG = hc_plt.plot_single_DEL_Scatter(
                DATA_OUT["Validation"], INPUT["Validation"], LIMS, num_curr - 1)
            if "pdf" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_pdf(FIG, path_out + f'Validation_scatter_single_sec_{num_curr}')

            if "png" in INPUT_SELECT["plot_as"]:
                hc_plt.save_figs_as_png(FIG, path_out + f'Validation_scatter_single_sec_{num_curr}')
# </editor-fold>

# <editor-fold desc="Main_CSV Out">


# %% Main_CSV Out
if INPUT["SELECT"]["CSV_out"]:

    path_csv = path_out + 'csv_data/'
    if not os.path.exists(path_csv):
        os.makedirs(path_csv)

    print("exporting CSV Data...")

    if "HSTP" in DATA_OUT:
        print("    HSTP Data")

        Save_temp = {CK: CD['grid'] for CK, CD in DATA_OUT['HSTP'].items()}

        # Data_out_csv(Save_temp, "HSTP", path_csv)
        Data_out_xls(Save_temp, "HSTP_Overview.xlsx", path_csv)

    if "VMHS" in DATA_OUT:
        print("    VMHS Data")

        Save_temp = {CK: CD['grid'] for CK, CD in DATA_OUT['VMHS'].items()}

        # Data_out_csv(Save_temp, "VMHS", path_csv)
        Data_out_xls(Save_temp, "VMHS_Overview.xlsx", path_csv)

    if "VMTP" in DATA_OUT:
        print("    VMTP Data")

        Save_temp = {CK: CD['grid'] for CK, CD in DATA_OUT['VMTP'].items()}

        # Data_out_csv(Save_temp, "VMTP", path_csv)
        Data_out_xls(Save_temp, "VMTP_Overview.xlsx", path_csv)

    if "Tables" in DATA_OUT:
        print("    Tables Data")
        Save_temp = {CK: CD for CK, CD in DATA_OUT['Tables']['VMHS'].items() if isinstance(CD, pd.DataFrame)}
        Data_out_xls(Save_temp, "Table_VMHS.xlsx", path_csv)

        Save_temp = {CK: CD for CK, CD in DATA_OUT['Tables']['VMTP'].items() if isinstance(CD, pd.DataFrame)}
        Data_out_xls(Save_temp, "Table_VMTP.xlsx", path_csv)

    if "RosePlot" in DATA_OUT:
        print("    RosePlot Data")
        Save_temp = {CK: CD['grid'] for CK, CD in DATA_OUT['RosePlot'].items()}

        Data_out_xls(Save_temp, "RosePlot.xlsx", path_csv)

    if "RWI" in DATA_OUT:
        print("    RWI Data")
        Save_temp = {CK: pd.concat([cd_data, cd_rwi], axis=1) for (CK, cd_rwi), (_, cd_data) in
                     zip(DATA_OUT['RWI'].items(), DATA_SEC.items())}

        Data_out_xls(Save_temp, "RWI.xlsx", path_csv)

    if "WaveBreak_Steep" in DATA_OUT:
        print("    WaveBreak_Steep Data")
        Save_temp = {CK: pd.concat([cd_data, cd_rwi], axis=1) for (CK, cd_rwi), (_, cd_data) in
                     zip(DATA_OUT['WaveBreak_Steep'].items(), DATA_SEC.items())}

        Data_out_xls(Save_temp, "WaveBreak_Steep.xlsx", path_csv)

    if "Validation" in DATA_OUT:
        print("    validation Data (pickled, see 'Tipps_ReadMe')")
        Save_temp = DATA_OUT["Validation"]
        with open(path_csv + 'Validation.pkl', 'wb') as valid_file:
            pickle.dump(Save_temp, valid_file)
# </editor-fold>

# <editor-fold desc="Main_saveInfolog">
# %% Main_saveInfolog
print("saving Log...")

with open(path_out + 'Info_LOG.txt', "w") as log_text:
    log_text.write(INFO_LOG)
del log_text

print(f"{script_name} finished!")
# </editor-fold>
